AWSTemplateFormatVersion: '2010-09-09'

Description: >-
  Creates resources specific to running Cromwell on AWS

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Cromwell Configuration
        Parameters:
          - PipelineName
          - InputDataLocation 
          - OutputDataLocation
      - Label:
          default: Head Node Configuration
        Parameters:
          - InstanceType
          - HeadNodeEBSVolumeSize
          - KeyPair
          - AllowedSSHLocation
      - Label:
          default: Batch Configuration
        Parameters:
          - VpcId
          - WorkerNodeSubnetId
          - ComputeEnvMinvCpus
          - ComputeEnvMaxvCpus
          - SpotBidPercentage
          - WorkerNodeInstanceType
          - WorkerNodeEBSVolumeSize
          - ResearcherName
          - ProjectId
    ProductName: cromwell

Parameters:
  Namespace:
    Type: String
    Description: Optional namespace (e.g. project name) to use to label resources. If not specified the stack-name will be used.
    Default: ""
  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: 'The VPC to create security groups and deploy AWS Batch to. NOTE: Must be the same VPC as the provided subnet IDs.'
  WorkerNodeSubnetId:
    Type: AWS::EC2::Subnet::Id
    Description: 'Subnets you want your batch compute environment to launch in. We recommend public subnets. NOTE: Must be from the VPC provided.'
  KeyPair:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the Head Node. Please create a key pair from the key pair screen if they are not available in the dropdown.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Must be the name of an existing EC2 KeyPair.
  PipelineName:
    Type: String
    Description: Search and select the pipeline git repository URL. If not found please enter the custom pipeline URL.
  InputDataLocation:
    Description: >-
      An S3 bucket path which holds input data for the cromwell pipeline. Eg: bucket-name/prefix
    Type: String 
  OutputDataLocation:
    Description: >-
      An S3 bucket path where the output of the cromwell job gets stored. Eg: bucket-name/prefix
    Type: String
  ComputeEnvMinvCpus:
    Description: The minimum number of CPUs to be kept in running state for the Batch Worker Nodes. If you give a non-zero value, some worker nodes may stay in running state always and you may incur higher cost.
    Type: Number
    Default: 0
  ComputeEnvMaxvCpus:
    Description: The maximum number of CPUs for the default Batch Compute Environment
    Type: Number
    Default: 100
  HeadNodeEBSVolumeSize:
    Description: The initial size of the volume (in GBs) Head Node EBS will use for storage.
    Type: Number
    Default: 16
  WorkerNodeEBSVolumeSize:
    Description: The initial size of the volume (in GBs) Worker Node EBS will use for storage.
    Type: Number
    Default: 100
  SpotBidPercentage:
    Type: Number
    Description: The maximum percentage of On-Demand pricing you want to pay for Spot resources. You will always pay the lowest Spot market price and never more than your maximum percentage.
    Default: 100
  InstanceType:
    Description: Head Node EC2 instance type
    Type: String
    Default: t3.small
    AllowedValues: [t3.nano, t3.micro, t3.small, t3.medium]
    ConstraintDescription: Must be a valid EC2 instance type.
  WorkerNodeInstanceType:
    Description: Specify the instance types to be used to carry out the computation. You can specify one or more family or instance type. The option 'optimal' chooses the best fit of M4, C4, and R4 instance types available in the region.
    Type: String
    Default: optimal
    AllowedValues: [optimal, c4.large, m4.large, r4.large, c4.4xlarge, m4.4xlarge, r4.4xlarge]
  AllowedSSHLocation:
    Description: The IP address range that can be used to SSH to the Head Node
    Type: String
    MinLength: '9'
    MaxLength: '18'
    Default: 0.0.0.0/0
    AllowedPattern: (\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})/(\d{1,2})
    ConstraintDescription: Must be a valid IP CIDR range of the form x.x.x.x/x.
  LatestAmiId:
    Type:  'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2'
  ResearcherName:
    Type: String
  ProjectId:
    Type: String

Resources:
  EC2LaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: !Sub cromwell-launch-tp-${Namespace}
      LaunchTemplateData:
        # Used in tandem with UserData to check if the instance is provisioned
        # correctly. It is important to terminate mis-provisioned instances before
        # jobs are placed on them
        InstanceInitiatedShutdownBehavior: terminate
        TagSpecifications:
          - ResourceType: instance
            Tags:
            - Key: cost_resource
              Value: !Sub ${AWS::StackName}
            - Key: Name
              Value: !Join ['-', [Ref: Namespace, 'cromwell-work-node']]
            - Key: researcher_name
              Value: !Ref ResearcherName
            - Key: project_name
              Value: !Ref ProjectId
          - ResourceType: volume
            Tags:
            - Key: cost_resource
              Value: !Sub ${AWS::StackName}
            - Key: Name
              Value: !Join ['-', [Ref: Namespace, 'cromwell-work-node']]
            - Key: researcher_name
              Value: !Ref ResearcherName
            - Key: project_name
              Value: !Ref ProjectId
        BlockDeviceMappings:
          - Ebs:
              DeleteOnTermination: True
              VolumeSize: !Ref WorkerNodeEBSVolumeSize
              VolumeType: gp2
            DeviceName: /dev/xvda
          - Ebs:
              Encrypted: True
              DeleteOnTermination: True
              VolumeSize: 22
              VolumeType: gp2
            DeviceName: /dev/xvdcz
          - Ebs:
              Encrypted: True
              DeleteOnTermination: True
              VolumeSize: 100
              VolumeType: gp2
            DeviceName: /dev/xvdba
        EbsOptimized: true
        UserData:
          Fn::Base64:
            Fn::Sub: |
              MIME-Version: 1.0
              Content-Type: multipart/mixed; boundary="==BOUNDARY=="

              --==BOUNDARY==
              Content-Type: text/cloud-config; charset="us-ascii"

              #cloud-config
              repo_update: true
              repo_upgrade: security

              packages:
              - jq
              - btrfs-progs
              - sed
              - git
              - amazon-ssm-agent
              - unzip
              - wget
              - amazon-cloudwatch-agent

              write_files:
                - permissions: '0644'
                  path: /tmp/provision.sh
                  content: |
                    #!/bin/bash
                    set -e
                    set -x
                    BASEDIR=`dirname $0`
                    # start ssm-agent
                    systemctl enable amazon-ssm-agent
                    systemctl start amazon-ssm-agent
                    function ecs() {
                        case $1 in
                            disable)
                                systemctl stop ecs
                                systemctl stop docker
                                ;;
                            enable)
                                systemctl start docker
                                systemctl enable --now --no-block ecs  # see: https://github.com/aws/amazon-ecs-agent/issues/1707
                                ;;
                        esac
                    }
                    # make sure that docker and ecs are running on script exit to avoid
                    # zombie instances
                    trap "ecs enable" INT ERR EXIT
                    set +e
                    ecs disable
                    set -e
                    # common provisioning for all workflow orchestrators
                    cd /opt
                    sh $BASEDIR/ecs-additions-common.sh
              
              runcmd:

              # install aws-cli v2 and copy the static binary in an easy to find location for bind-mounts into containers
              - curl -s "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
              - unzip -q /tmp/awscliv2.zip -d /tmp
              - /tmp/aws/install -b /usr/bin

              # check that the aws-cli was actually installed. if not shutdown (terminate) the instance
              - command -v aws || shutdown -P now

              - mkdir -p /opt/aws-cli/bin
              - cp -a $(dirname $(find /usr/local/aws-cli -name 'aws' -type f))/. /opt/aws-cli/bin/

              # enable ecs spot instance draining
              - echo ECS_ENABLE_SPOT_INSTANCE_DRAINING=true >> /etc/ecs/ecs.config

              # pull docker images only if missing
              - echo ECS_IMAGE_PULL_BEHAVIOR=prefer-cached >> /etc/ecs/ecs.config

              # pull artifcats              
              - cd /opt
              - wget https://aws-genomics-workflows.s3.amazonaws.com/v3.0.7/artifacts/aws-ecs-additions.zip
              - unzip aws-ecs-additions.zip -d ecs-additions && rm -f aws-ecs-additions.zip
              
              # Replace provision script with custom script
              - mv -f /tmp/provision.sh /opt/ecs-additions/provision.sh
              - chmod a+x /opt/ecs-additions/provision.sh
              - /opt/ecs-additions/provision.sh

              --==BOUNDARY==--
  
  S3Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Properties:
      BucketName: !Sub cromwell-work-data-${Namespace}
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
             SSEAlgorithm: AES256
      Tags:
        - Key: cost_resource
          Value: !Sub ${AWS::StackName}
  
  S3CleanupLambdaInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub s3cleanup-lambda-role-${Namespace}
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Sid: S3BucketAllowAllObjectOps
                Effect: Allow
                Resource: 
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                Action:
                  - "s3:*"

  S3CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub s3cleanup-lambda-${Namespace}
      Code: 
        ZipFile: 
          !Sub |
            import json, boto3, logging
            import cfnresponse
            logger = logging.getLogger()
            logger.setLevel(logging.INFO)

            def lambda_handler(event, context):
                logger.info("event: {}".format(event))
                try:
                    bucket = event['ResourceProperties']['BucketName']
                    logger.info("bucket: {}, event['RequestType']: {}".format(bucket,event['RequestType']))
                    if event['RequestType'] == 'Delete':
                        s3 = boto3.resource('s3')
                        bucket = s3.Bucket(bucket)
                        for obj in bucket.objects.filter():
                            logger.info("delete obj: {}".format(obj))
                            s3.Object(bucket.name, obj.key).delete()

                    sendResponseCfn(event, context, cfnresponse.SUCCESS)
                except Exception as e:
                    logger.info("Exception: {}".format(e))
                    sendResponseCfn(event, context, cfnresponse.FAILED)

            def sendResponseCfn(event, context, responseStatus):
                responseData = {}
                responseData['Data'] = {}
                cfnresponse.send(event, context, responseStatus, responseData, "CustomResourcePhysicalID")            

      Handler: "index.lambda_handler"
      Runtime: python3.7
      MemorySize: 128
      Timeout: 900
      Role: !GetAtt S3CleanupLambdaInstanceRole.Arn
    DependsOn: S3CleanupLambdaInstanceRole  
  
  S3CleanUpBucketOnDelete:
    Type: Custom::cleanupbucket
    Properties:
      ServiceToken: !GetAtt S3CleanupLambdaFunction.Arn
      BucketName: !Ref S3Bucket
    DependsOn: S3Bucket
  
  BatchJobRole:
    Type: AWS::IAM::Role
    Properties:
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Deny
                Resource:
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                Action:
                  - "s3:Delete*"
                  - "s3:PutBucket*"
              - Effect: Allow
                Resource: !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                Action:
                  - "s3:ListBucket*"
              - Effect: Allow
                Resource: !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                Action:
                  - "s3:*"
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ecs-tasks.amazonaws.com"
            Action:
              - "sts:AssumeRole"
  
  BatchInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ec2.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub S3Bucket-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Sid: S3BucketAllowAllObjectOps
                Effect: Allow
                Resource: 
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket]]
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                  - !Sub
                    - 'arn:aws:s3:::${InputS3Bucket}'
                    - {InputS3Bucket: !Select [0, !Split ["/", !Ref InputDataLocation ]]}
                  - !Sub
                    - 'arn:aws:s3:::${InputS3Bucket}/*'
                    - {InputS3Bucket: !Select [0, !Split ["/", !Ref InputDataLocation ]]}
                  - !Sub
                    - 'arn:aws:s3:::${OutputS3Bucket}'
                    - {OutputS3Bucket: !Select [0, !Split ["/", !Ref OutputDataLocation ]]}
                  - !Sub
                    - 'arn:aws:s3:::${OutputS3Bucket}/*'
                    - {OutputS3Bucket: !Select [0, !Split ["/", !Ref OutputDataLocation ]]}
                Action:
                  - "s3:*"
 
        # required for amazon-ebs-autoscale to resize filesystems
        - PolicyName: !Sub Autoscale-EBS-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              Effect: Allow
              Action:
                - "ec2:createTags"
                - "ec2:createVolume"
                - "ec2:attachVolume"
                - "ec2:deleteVolume"
                - "ec2:modifyInstanceAttribute"
                - "ec2:describeVolumes"
              Resource: "*"
                
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role"
      - "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      - "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
  
  BatchInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - Ref: BatchInstanceRole
  
  BatchSpotFleetRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "spotfleet.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
      - "arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole"
  
  BatchServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: batch.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: SG for cromwell workflows on Batch
      VpcId:
        Ref: VpcId
  
  SpotComputeEnv:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      ComputeEnvironmentName: !Sub cromwell-worker-spot-${Namespace}
      ServiceRole: !GetAtt BatchServiceRole.Arn
      Type: MANAGED
      State: ENABLED
      ComputeResources:
        Ec2Configuration:
          - ImageType: ECS_AL2
        AllocationStrategy: SPOT_CAPACITY_OPTIMIZED
        # Set the Spot price to 100% of on-demand price
        # This is the maximum price for spot instances that Batch will launch.
        # Lowering this puts a limit on the spot capacity that Batch has available.
        # Spot instances are terminated when on-demand capacity is needed, regardless of the price set.
        BidPercentage: !Ref SpotBidPercentage 
        # Ec2KeyPair: !Ref Ec2KeyPairName
        LaunchTemplate:
          LaunchTemplateId: !Ref EC2LaunchTemplate
          Version: $Latest
        InstanceRole: !GetAtt BatchInstanceProfile.Arn
        InstanceTypes: 
          - !Ref WorkerNodeInstanceType
        MinvCpus: !Ref ComputeEnvMinvCpus
        MaxvCpus: !Ref ComputeEnvMaxvCpus
        SecurityGroupIds:
          - !Ref SecurityGroup
        SpotIamFleetRole: !GetAtt BatchSpotFleetRole.Arn
        Subnets:
          - !Ref WorkerNodeSubnetId
        Type: SPOT
        Tags:
          Name: !Sub batch-worker-spot-${Namespace}

  DefaultQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      JobQueueName: !Sub worker-node-queue-${Namespace}
      Tags: 
        Name: !Sub worker-node-queue-${Namespace} 
      Priority: 1
      State: ENABLED
      ComputeEnvironmentOrder:
        - Order: 1
          ComputeEnvironment: !Ref SpotComputeEnv

  Ec2InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub cromwell-head-node-role-${Namespace}
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      Policies:
        - PolicyName: !Sub CromwellServer-S3-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Resource:
                  - !Join ["", ["arn:aws:s3:::", !Ref S3Bucket, "/*"]]
                  - !Sub
                    - 'arn:aws:s3:::${InputS3Bucket}/*'
                    - {InputS3Bucket: !Select [0, !Split ["/", !Ref InputDataLocation ]]}
                  - !Sub
                    - 'arn:aws:s3:::${OutputS3Bucket}/*'
                    - {OutputS3Bucket: !Select [0, !Split ["/", !Ref OutputDataLocation ]]}
                Action:
                  - "s3:*"
              - Effect: Allow
                Resource: "*"
                Action:
                  - "s3:ListBucket"
                  - "s3:ListAllMyBuckets"
        - PolicyName: !Sub CromwellServer-BatchQueue-Access-${AWS::Region}
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              Effect: Allow
              Resource: "*"
              Action:
                - "batch:DescribeJobQueues"
                - "batch:DeregisterJobDefinition"
                - "batch:TerminateJob"
                - "batch:DescribeJobs"
                - "batch:CancelJob"
                - "batch:SubmitJob"
                - "batch:RegisterJobDefinition"
                - "batch:DescribeJobDefinitions"
                - "batch:ListJobs"
                - "batch:DescribeComputeEnvironments"
                - "ecs:DescribeContainerInstances"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
  
  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: "/"
      Roles:
        - Ref: Ec2InstanceRole
  
  EC2SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Cromwell server access via SSH and HTTP/HTTPS
      VpcId: !Ref VpcId
      Tags:
        - Key: Name
          Value: !Sub cromwell-server-sg-${Namespace}
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 8000
          ToPort: 8000
          CidrIp: !Ref 'AllowedSSHLocation'
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: !Ref 'AllowedSSHLocation'

  EC2Instance:
    Type: AWS::EC2::Instance
    CreationPolicy:
      ResourceSignal:
        Timeout: PT15M
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - W4002
      AWS::CloudFormation::Init:
        configSets:
          default:
            - config1
            - config2
            - config3
            - config4

        config1:
          files:            
            "/home/ec2-user/get_cromwell.sh":
              content: !Sub |
                #!/bin/bash
                url=$(curl --silent --retry 5 --retry-connrefused https://api.github.com/repos/broadinstitute/cromwell/releases/tags/73 | jq -r .assets[0].browser_download_url)
                if [[ $url == s3://* ]]; then
                  aws s3 cp $url .
                else
                  curl --retry 5 --retry-connrefused -LO $url
                fi
                ln -s $(find . | grep "cromwell.*\.jar") cromwell.jar

              mode: "000755"
              owner: "ec2-user"
              group: "ec2-user"

            "/home/ec2-user/get_cromwell_tools.sh":
              content: |
                #!/bin/bash
                url=$(curl --silent --retry 5 --retry-connrefused https://api.github.com/repos/broadinstitute/cromwell/releases/latest | jq -r .assets[1].browser_download_url)
                curl --retry 5 --retry-connrefused -LO $url

                curl --retry 5 --retry-connrefused -LO https://raw.githubusercontent.com/broadinstitute/cromshell/master/cromshell
                chmod +x cromshell
                sudo mv cromshell /usr/local/bin/

              mode: "000755"
              owner: "ec2-user"
              group: "ec2-user"

            "/home/ec2-user/cromwell.conf":
              content: !Sub |
                  include required(classpath("application"))

                  webservice {
                    interface = localhost
                    port = 8000
                  }

                  akka {
                    http {
                      server {
                        request-timeout = 300s
                        idle-timeout = 300s
                      }
                    }
                  }

                  system {
                    job-rate-control {
                      jobs = 1
                      per = 2 second
                    }
                  }

                  aws {
                    application-name = "cromwell"
                    auths = [{
                        name = "default"
                        scheme = "default"
                    }]
                    region = "${AWS::Region}"
                  }

                  call-caching {
                    enabled = true
                    invalidate-bad-cache-results = true
                  }

                  engine { filesystems { s3 { auth = "default" } } }

                  backend {
                    default = "AWSBATCH"
                    providers {
                      AWSBATCH {
                        actor-factory = "cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory"
                        config {
                          numSubmitAttempts = 10
                          numCreateDefinitionAttempts = 10
                          root = "s3://${OutputDataLocation}"
                          auth = "default"
                          default-runtime-attributes { queueArn = "${DefaultQueue}" , scriptBucketName = "${S3Bucket}" }
                          filesystems { 
                            s3 { 
                              auth = "default"
                              duplication-strategy: [
                                "hard-link", "soft-link", "copy"
                              ]
                            } 
                          }
                        }
                      }
                    }
                  }
              mode: "000644"
              owner: "ec2-user"
              group: "ec2-user"

            "/home/ec2-user/supervisord.conf":
              mode: "000644"
              owner: "ec2-user"
              group: "ec2-user"
              content: |
                [unix_http_server]
                file=/home/ec2-user/supervisor.sock   ; the path to the socket file
                
                [supervisord]
                logfile=/home/ec2-user/supervisord.log ; main log file; default $CWD/supervisord.log
                logfile_maxbytes=50MB                  ; max main logfile bytes b4 rotation; default 50MB
                logfile_backups=10                     ; # of main logfile backups; 0 means none, default 10
                loglevel=info                          ; log level; default info; others: debug,warn,trace
                pidfile=/home/ec2-user/supervisord.pid ; supervisord pidfile; default supervisord.pid
                nodaemon=false                         ; start in foreground if true; default false
                minfds=1024                            ; min. avail startup file descriptors; default 1024
                minprocs=200                           ; min. avail process descriptors;default 200
                
                [rpcinterface:supervisor]
                supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

                [supervisorctl]
                serverurl=unix:///home/ec2-user/supervisor.sock ; use a unix:// URL  for a unix socket

                [program:cromwell-server]
                command=java -Dconfig.file=cromwell.conf -XX:MaxRAMPercentage=85.0 -jar cromwell.jar server  ; the program (relative uses PATH, can take args)
                directory=/home/ec2-user                                                                     ; directory to cwd to before exec (def no cwd)
                user=ec2-user                                                                                ; setuid to this UNIX account to run the program
                redirect_stderr=true                                                                         ; redirect proc stderr to stdout (default false)
                stdout_logfile=/home/ec2-user/cromwell-server.log                                            ; stdout log path, NONE for none; default AUTO
          
            "/etc/nginx/nginx.conf":
              mode: "000644"
              content: |
                # For more information on configuration, see:
                #   * Official English Documentation: http://nginx.org/en/docs/
                #   * Official Russian Documentation: http://nginx.org/ru/docs/

                user nginx;
                worker_processes auto;
                error_log /var/log/nginx/error.log;
                pid /var/run/nginx.pid;

                # Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.
                include /usr/share/nginx/modules/*.conf;

                events {
                    worker_connections 1024;
                }

                http {
                    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                                      '$status $body_bytes_sent "$http_referer" '
                                      '"$http_user_agent" "$http_x_forwarded_for"';

                    access_log  /var/log/nginx/access.log  main;

                    sendfile            on;
                    tcp_nopush          on;
                    tcp_nodelay         on;
                    keepalive_timeout   65;
                    types_hash_max_size 2048;

                    include             /etc/nginx/mime.types;
                    default_type        application/octet-stream;

                    # Load modular configuration files from the /etc/nginx/conf.d directory.
                    # See http://nginx.org/en/docs/ngx_core_module.html#include
                    # for more information.
                    include /etc/nginx/conf.d/*.conf;

                    index   index.html index.htm;

                    server {
                        # Config options for a TLS enabled server
                        listen       80;
                        listen       443 ssl http2 default_server;
                        listen       [::]:443 ssl http2 default_server;
                        server_name  localhost;
                        root         /usr/share/nginx/html;
                
                        ssl_certificate "/etc/pki/tls/certs/server.crt";
                        ssl_certificate_key "/etc/pki/tls/certs/server.key";
                        
                        # It is *strongly* recommended to generate unique DH parameters
                        # Generate them with: openssl dhparam -out /etc/pki/nginx/dhparams.pem 2048
                        #ssl_dhparam "/etc/pki/nginx/dhparams.pem";
                        ssl_session_cache shared:SSL:1m;
                        ssl_session_timeout  10m;
                
                        # Load configuration files for the default server block.
                        include /etc/nginx/default.d/*.conf;

                        # redirect all non-ssl traffic to ssl
                        if ($ssl_protocol = "") {
                          rewrite ^ https://$host$request_uri? permanent;
                        }

                        location / {
                            proxy_pass    http://localhost:8000;
                        }

                        # redirect server error pages to the static page /40x.html
                        #
                        error_page 404 /404.html;
                            location = /40x.html {
                        }

                        # redirect server error pages to the static page /50x.html
                        #
                        error_page 500 502 503 504 /50x.html;
                            location = /50x.html {
                        }
                    }
                }

            "/home/ec2-user/supervisord.log":
              content: "\n"
              mode: "000664"
              owner: "ec2-user"
              group: "ec2-user"

            "/home/ec2-user/.aws/config":
              content: !Sub |
                [default]
                region = ${AWS::Region}

              mode: "000644"
              owner: "ec2-user"
              group: "ec2-user"

        config2:
          commands:            
            02_enable_nginx:
              command: "systemctl enable nginx"
            
            03_start_nginx:
              command: "systemctl start nginx"

        config3:
          commands:
            00_get_cromwell:
              cwd: "/home/ec2-user/"
              command: "./get_cromwell.sh"
            
            01_chown_cromwell:
              cwd: "/home/ec2-user/"
              command: "chown ec2-user:ec2-user cromwell*.jar"
                        
            02_start_cromwell:
              cwd: "/home/ec2-user"
              command: "sudo -u ec2-user /usr/local/bin/supervisord"

        config4:
          commands:
            00_get_cromwell_tools:
              cwd: "/home/ec2-user/"
              command: "./get_cromwell_tools.sh"
            
            01_chown_womtool:
              cwd: "/home/ec2-user/"
              command: "chown ec2-user:ec2-user womtool*.jar"
    Properties:
      ImageId: !Ref 'LatestAmiId'
      InstanceType: !Ref InstanceType
      IamInstanceProfile: !Ref EC2InstanceProfile
      PropagateTagsToVolumeOnCreation: true
      SecurityGroupIds:
        - { "Fn::GetAtt" : ["EC2SecurityGroup", "GroupId"] }
      KeyName: !Ref 'KeyPair'
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref HeadNodeEBSVolumeSize
            Encrypted: true
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -x
          yum -y update
          yum install -y aws-cfn-bootstrap
          yum install -y jq awslogs python3 java mailx git docker

          sudo systemctl enable docker.service
          sudo systemctl enable containerd.service
          sudo systemctl start docker
          sudo usermod -a -G docker ec2-user

          amazon-linux-extras install -y nginx1
          pip3 install supervisor

          # create a self-signed certificate
          cd /tmp
          openssl genrsa 2048 > server.key
          openssl req -new -key server.key -out csr.pem -subj "/C=US/ST=WA/L=Seattle/O=anon/OU=anon/CN=selfsigned/emailAddress=selfsigned"
          openssl x509 -req -days 365 -in csr.pem -signkey server.key -out server.crt
          cp server.crt server.key /etc/pki/tls/certs/
          rm -f server.crt server.key csr.pem

          # Pull repository and images
          cd /home/ec2-user/
          git clone ${PipelineName} workflow

          # Set supervisor to start on boot
          sudo crontab -l 2>/dev/null > "/tmp/crontab"
          echo '@reboot cd /home/ec2-user && sudo -u ec2-user /usr/local/bin/supervisord 2>&1 >> /var/log/cromwell-start.log' >> "/tmp/crontab"
          sudo crontab "/tmp/crontab"

          /opt/aws/bin/cfn-init --verbose --stack ${AWS::StackName} --resource EC2Instance --region ${AWS::Region}
          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource EC2Instance --region ${AWS::Region}
      Tags:
        - Key: Name
          Value: !Join ['-', [Ref: Namespace, 'cromwell-head-node']]
        - Key: cost_resource
          Value: !Ref "AWS::StackName"
        - Key: Description
          Value: EC2 workspace instance
  
Outputs:
  InstanceId:
    Description: InstanceId of the newly created EC2 instance
    Value: !Ref 'EC2Instance'
  InstanceIPAddress:
    Description: IP address of the newly created EC2 instance
    Value: !GetAtt [EC2Instance, PublicIp]
  InstanceDNSName:
    Description: DNS name of the newly created EC2 instance
    Value: !GetAtt [EC2Instance, PublicDnsName]
  LaunchTemplateId:
    Description: >-
      EC2 Launch Template ID to use when creating AWS Batch compute environments
    Value: !Ref EC2LaunchTemplate
  WorkDataLocation:
    Value: !Sub ${S3Bucket}
  BatchComputeEnvironment:
    Value: !Ref SpotComputeEnv
